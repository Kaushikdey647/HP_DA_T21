{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.8.0-cp310-cp310-manylinux2010_x86_64.whl (497.6 MB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (1.22.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (1.1.0)\n",
      "Collecting tensorboard<2.9,>=2.8\n",
      "  Using cached tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (1.44.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3.10/site-packages (from tensorflow) (57.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Collecting keras<2.9,>=2.8.0rc0\n",
      "  Using cached keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/huntrag/.local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/huntrag/.local/lib/python3.10/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/huntrag/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/huntrag/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/huntrag/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/huntrag/.local/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/huntrag/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/huntrag/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/huntrag/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/huntrag/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/huntrag/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/huntrag/.local/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
      "Installing collected packages: tensorboard, keras, tensorflow\n",
      "Successfully installed keras-2.8.0 tensorboard-2.8.0 tensorflow-2.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-19 19:11:36.382100: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-19 19:11:36.382121: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-19 19:11:39.444342: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-03-19 19:11:39.444609: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-19 19:11:39.444696: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-03-19 19:11:39.444766: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-03-19 19:11:39.444830: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-03-19 19:11:39.444891: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-03-19 19:11:39.444953: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-03-19 19:11:39.445031: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-03-19 19:11:39.445100: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-03-19 19:11:39.445110: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-03-19 19:11:39.445681: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 134ms/step - loss: 179105.4688 - val_loss: 39753.5273\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 178705.9219 - val_loss: 39558.0625\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 178294.7656 - val_loss: 39364.4023\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 177905.3594 - val_loss: 39169.3125\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 177513.4531 - val_loss: 38974.4961\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 177111.6875 - val_loss: 38781.9883\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 176707.2031 - val_loss: 38591.1953\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 176320.5625 - val_loss: 38399.0039\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 175925.3594 - val_loss: 38207.4375\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 175526.6406 - val_loss: 38017.1953\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 175140.4531 - val_loss: 37826.1250\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 174738.3594 - val_loss: 37637.3945\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 174353.3906 - val_loss: 37447.8477\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 173965.5938 - val_loss: 37258.3242\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 173574.8125 - val_loss: 37070.0195\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 173179.5625 - val_loss: 36883.5273\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 172786.3594 - val_loss: 36698.3984\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 172400.9531 - val_loss: 36513.0469\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 172011.7656 - val_loss: 36328.4961\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 171627.7500 - val_loss: 36143.9844\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 171245.7500 - val_loss: 35959.0273\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 170863.0625 - val_loss: 35774.4102\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 170476.0625 - val_loss: 35591.1094\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 170087.1406 - val_loss: 35409.5156\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 169703.2969 - val_loss: 35228.6641\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 169325.4375 - val_loss: 35047.3633\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 168938.0156 - val_loss: 34867.8086\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 168563.7812 - val_loss: 34687.2031\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 168184.1719 - val_loss: 34507.1133\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 167794.7031 - val_loss: 34329.7227\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 167420.6406 - val_loss: 34151.6641\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 167037.1094 - val_loss: 33975.2148\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 166667.7344 - val_loss: 33797.2734\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 166281.6406 - val_loss: 33621.8906\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 165913.2656 - val_loss: 33445.0352\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 165541.1094 - val_loss: 33268.1367\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 165154.4531 - val_loss: 33094.4414\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 164778.8594 - val_loss: 32921.2539\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 164411.7812 - val_loss: 32746.9824\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 164037.3125 - val_loss: 32573.6543\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 163672.6094 - val_loss: 32399.4316\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 163286.4375 - val_loss: 32229.1250\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 162920.9844 - val_loss: 32058.0742\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 162549.4844 - val_loss: 31887.6426\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 162181.1875 - val_loss: 31717.4434\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 161816.2500 - val_loss: 31547.1562\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 161444.5156 - val_loss: 31378.7051\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 161080.8125 - val_loss: 31210.2695\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 160701.1875 - val_loss: 31045.1953\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 160346.1406 - val_loss: 30877.8555\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 159998.0312 - val_loss: 30707.8555\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 159615.1562 - val_loss: 30542.4141\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 159247.9531 - val_loss: 30378.3184\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 158895.0625 - val_loss: 30212.6582\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 158519.8594 - val_loss: 30050.4551\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 158160.6094 - val_loss: 29887.8242\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 157807.9844 - val_loss: 29723.8828\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 157441.9375 - val_loss: 29561.5430\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 157084.9844 - val_loss: 29399.0000\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 156727.5625 - val_loss: 29236.7617\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 156366.9844 - val_loss: 29075.4922\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 156015.8125 - val_loss: 28913.5234\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 155651.3906 - val_loss: 28753.8750\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 155287.4062 - val_loss: 28596.3926\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 154942.9062 - val_loss: 28437.0312\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 154583.9844 - val_loss: 28279.0391\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 154225.7344 - val_loss: 28122.1582\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 153874.7344 - val_loss: 27965.0293\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 153530.0625 - val_loss: 27806.7324\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 153165.3906 - val_loss: 27651.7188\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 152814.6094 - val_loss: 27496.9004\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 152468.6719 - val_loss: 27341.2109\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 152106.8594 - val_loss: 27188.5059\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 151770.4844 - val_loss: 27033.1016\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 151415.4531 - val_loss: 26879.4434\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 151069.6250 - val_loss: 26725.6582\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 150715.5938 - val_loss: 26573.8359\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 150369.2656 - val_loss: 26422.0742\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 150025.3594 - val_loss: 26270.1934\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 149679.7812 - val_loss: 26118.9395\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 149333.5469 - val_loss: 25968.4141\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 148992.5156 - val_loss: 25817.8047\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 148642.0312 - val_loss: 25669.2441\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 148301.5625 - val_loss: 25520.6875\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 147951.9531 - val_loss: 25374.3301\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 147605.7188 - val_loss: 25229.2363\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 147276.2812 - val_loss: 25081.8125\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 146937.8594 - val_loss: 24934.3262\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 146596.0625 - val_loss: 24787.7734\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 146251.6094 - val_loss: 24642.9180\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 145919.9531 - val_loss: 24497.2988\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 145580.2031 - val_loss: 24352.7559\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 145235.5625 - val_loss: 24210.3242\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 144904.9062 - val_loss: 24067.1875\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 144571.3750 - val_loss: 23924.2109\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 144232.2344 - val_loss: 23782.7578\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 143906.4062 - val_loss: 23640.0781\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 143556.2344 - val_loss: 23501.2422\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 143237.7188 - val_loss: 23359.7070\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 142906.8594 - val_loss: 23218.3984\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 142561.0156 - val_loss: 23080.2617\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 142232.3906 - val_loss: 22942.2148\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 141909.1094 - val_loss: 22803.3867\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 141575.9844 - val_loss: 22665.7480\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 141242.2344 - val_loss: 22529.3203\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 140913.5625 - val_loss: 22393.2344\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 140590.9531 - val_loss: 22256.5117\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 140254.7656 - val_loss: 22121.9375\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 139944.5156 - val_loss: 21984.4199\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 139603.8594 - val_loss: 21850.3613\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 139273.0938 - val_loss: 21717.7383\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 138956.2344 - val_loss: 21583.5957\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 138637.9062 - val_loss: 21448.7520\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 138307.7344 - val_loss: 21315.7832\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 137981.4062 - val_loss: 21184.1562\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 137659.6562 - val_loss: 21052.8027\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 137339.7812 - val_loss: 20921.5723\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 137017.0156 - val_loss: 20791.1074\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 136708.8594 - val_loss: 20658.7441\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 136376.1875 - val_loss: 20529.7246\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 136055.6406 - val_loss: 20401.6016\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 135742.1406 - val_loss: 20272.9961\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 135427.2969 - val_loss: 20144.3633\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 135107.3594 - val_loss: 20016.9395\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 134786.0000 - val_loss: 19891.0020\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 134470.5625 - val_loss: 19765.3262\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 134163.1094 - val_loss: 19638.5723\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 133836.9531 - val_loss: 19514.5957\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 133522.2031 - val_loss: 19390.8984\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 133222.9844 - val_loss: 19264.7148\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 132900.5000 - val_loss: 19140.7793\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 132586.6562 - val_loss: 19017.3418\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 132276.7812 - val_loss: 18893.8965\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 131956.0938 - val_loss: 18772.4316\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 131656.0469 - val_loss: 18649.0762\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 131350.5938 - val_loss: 18525.4297\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 131031.1016 - val_loss: 18404.3301\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 130714.6172 - val_loss: 18285.1875\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 130421.2500 - val_loss: 18163.8242\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 130104.7266 - val_loss: 18044.8145\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 129800.1172 - val_loss: 17925.8184\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 129493.0469 - val_loss: 17807.3125\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 129191.1172 - val_loss: 17688.6797\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 128876.7031 - val_loss: 17572.2676\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 128576.8438 - val_loss: 17455.3418\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 128269.9062 - val_loss: 17339.2930\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 127971.2812 - val_loss: 17222.6562\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 127662.6797 - val_loss: 17107.6445\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 127361.1562 - val_loss: 16992.7344\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 127069.8516 - val_loss: 16876.4238\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 126762.1562 - val_loss: 16761.9062\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 126448.1562 - val_loss: 16650.2852\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 126161.5938 - val_loss: 16536.4004\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 125856.0703 - val_loss: 16424.1309\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 125562.7578 - val_loss: 16311.2783\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 125265.0938 - val_loss: 16198.9053\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 124971.5234 - val_loss: 16086.3154\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 124665.2188 - val_loss: 15975.9219\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 124362.8438 - val_loss: 15866.8945\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 124084.6562 - val_loss: 15755.0479\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 123773.6328 - val_loss: 15646.6660\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 123483.2266 - val_loss: 15537.7959\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 123187.2734 - val_loss: 15429.5117\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 122903.2266 - val_loss: 15319.8906\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 122601.9531 - val_loss: 15212.2900\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 122310.9219 - val_loss: 15104.9023\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 122019.7500 - val_loss: 14997.8867\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 121722.4453 - val_loss: 14892.5596\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 121437.0312 - val_loss: 14786.9727\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 121142.2734 - val_loss: 14682.7275\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 120857.2422 - val_loss: 14577.9854\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 120566.1172 - val_loss: 14474.0713\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 120287.0234 - val_loss: 14368.8594\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 119993.3438 - val_loss: 14265.2441\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 119703.4922 - val_loss: 14162.7227\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 119418.2188 - val_loss: 14060.5000\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 119132.5312 - val_loss: 13958.7471\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 118847.3672 - val_loss: 13857.3154\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 118561.9297 - val_loss: 13756.3896\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 118277.6797 - val_loss: 13655.8398\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 117997.7734 - val_loss: 13555.2246\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 117707.7578 - val_loss: 13456.2910\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 117432.7734 - val_loss: 13356.5908\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 117146.2031 - val_loss: 13258.3096\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 116866.9297 - val_loss: 13160.2783\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 116588.1328 - val_loss: 13062.3467\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 116299.9062 - val_loss: 12966.2842\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 116037.1016 - val_loss: 12867.8115\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 115742.4062 - val_loss: 12772.3242\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 115473.3672 - val_loss: 12675.6631\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 115196.0000 - val_loss: 12579.3594\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 114914.8203 - val_loss: 12484.2383\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 114633.9297 - val_loss: 12390.2305\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 114361.3281 - val_loss: 12296.0381\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 114080.7031 - val_loss: 12202.9785\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 113807.7188 - val_loss: 12109.9570\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 113528.6328 - val_loss: 12017.9746\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 113267.6172 - val_loss: 11924.0947\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 112980.1172 - val_loss: 11832.9873\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 112708.0000 - val_loss: 11742.0840\n"
     ]
    }
   ],
   "source": [
    "#The following script is supposed to predict next ten stock prices\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def xy_data(dataset):\n",
    "    X_train=[]\n",
    "    y_train=[]\n",
    "    T = 5\n",
    "    for t in range(len(dataset)-T):\n",
    "        X_train.append(dataset[t:t+T])\n",
    "        y_train.append(dataset[t+T])\n",
    "    return X_train,y_train\n",
    "\n",
    "def create_model(X_train,y_train):\n",
    "    X_train = np.array(X_train).reshape(-1,5,1)\n",
    "    y_train = np.array(y_train)\n",
    "    # print(X_train.shape)\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.LSTM(5))\n",
    "    model.add(tf.keras.layers.Dense(1,activation = 'relu'))\n",
    "    model.compile(loss='mse',optimizer = tf.keras.optimizers.Adam(learning_rate=0.05))\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',patience=20,restore_best_weights=True)\n",
    "    n = list(X_train.shape)[0]\n",
    "    model.fit(X_train[int(0.2*n):],y_train[int(0.2*n):],epochs=200,validation_data=(X_train[:int(0.2*n)],y_train[:int(0.2*n)]),steps_per_epoch=5,callbacks=[callback])\n",
    "    return model\n",
    "    \n",
    "def predict(model,X):\n",
    "    pred = []\n",
    "    for _ in range(12):\n",
    "        # print(X[-5:])\n",
    "        data = np.array(X[-5:]).reshape(1,5,1)\n",
    "        # print(data.shape)\n",
    "        p = float(model.predict(data))\n",
    "        X.append(p)\n",
    "        pred.append(p)\n",
    "    return pred\n",
    "    \n",
    "def get_pred(Ticker,dataset):\n",
    "    ind = list(dataset['Ticker']).index(Ticker) \n",
    "    X_train,y_train = xy_data(list(dataset.transpose()[ind])[1:])\n",
    "    model = create_model(X_train,y_train)\n",
    "    predict_year = predict(model,list(dataset.transpose()[ind])[1:])\n",
    "    return predict_year\n",
    "\n",
    "def get_dict(preds):\n",
    "    y=2022\n",
    "    m=4\n",
    "    dates = {}\n",
    "    for i in range(12):\n",
    "        dates[f'{y}-{m}-01'] = preds[i]\n",
    "        if m==12:\n",
    "            m = 1\n",
    "            y +=1\n",
    "        else:\n",
    "            m+=1\n",
    "    return dates\n",
    "                                                             \n",
    "stockdata = pd.read_csv(\"../data/stockdatawith0.csv\")\n",
    "\n",
    "try:\n",
    "    preds = get_pred('adbe',stockdata)\n",
    "except:\n",
    "    preds=[]\n",
    "    print('Ticker not found')\n",
    "\n",
    "dict_pred = get_dict(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2022-4-01': 94.06629943847656,\n",
       " '2022-5-01': 94.06629943847656,\n",
       " '2022-6-01': 94.06629943847656,\n",
       " '2022-7-01': 94.06629943847656,\n",
       " '2022-8-01': 94.06629943847656,\n",
       " '2022-9-01': 94.06629943847656,\n",
       " '2022-10-01': 94.06629943847656,\n",
       " '2022-11-01': 94.06629943847656,\n",
       " '2022-12-01': 94.06629943847656,\n",
       " '2023-1-01': 94.06629943847656,\n",
       " '2023-2-01': 94.06629943847656,\n",
       " '2023-3-01': 94.06629943847656}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
